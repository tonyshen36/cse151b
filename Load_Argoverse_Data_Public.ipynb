{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from defs import ArgoverseDataset\n",
    "from defs import my_collate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a loader to enable batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the batch of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(4, 64, 3),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "            # torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Conv2d(64, 64*2, 3),\n",
    "            torch.nn.BatchNorm2d(64*2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2), \n",
    "            nn.Flatten(),\n",
    "            torch.nn.Linear(25088, 10028), # 18240\n",
    "            torch.nn.Dropout(0.3), # 0.5\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(10028, 5000), # 18240\n",
    "            torch.nn.Dropout(0.2), # 0.5\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(5000, 3600),\n",
    "            torch.nn.Dropout(0.1), # 0.2\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "def train(model, train_loader, device, optimizer, epoch, log_interval=10000):\n",
    "    model.train()\n",
    "    criterion = nn.MSELoss()\n",
    "    iterator = tqdm(train_loader, total=int(len(train_loader)))\n",
    "    counter = 0\n",
    "    # for i_batch, sample_batch in enumerate(train_loader):\n",
    "    for batch_idx, (inp, out) in enumerate(iterator):\n",
    "         \n",
    "        # inp, out = sample_batch\n",
    "        inp = inp.to(device)\n",
    "        out = out.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inp = inp.reshape(inp.shape[0], inp.shape[3], inp.shape[1], inp.shape[2])\n",
    "        pred_out = model(inp)\n",
    "\n",
    "        pred_out = pred_out.reshape(out.shape[0], out.shape[1] * out.shape[2], out.shape[3])\n",
    "        out = out.reshape(out.shape[0], out.shape[1] * out.shape[2], out.shape[3])\n",
    "        \n",
    "        loss = torch.sqrt(criterion(pred_out, out))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        counter += 1\n",
    "        iterator.set_postfix(loss=(loss.item()*inp.size(0) / (counter * train_loader.batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    criterion = nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        for i_batch, sample_batch in enumerate(test_loader):\n",
    "            inp, out = sample_batch\n",
    "            inp = inp.to(device)\n",
    "            out = out.to(device)\n",
    "            \n",
    "            inp = inp.reshape(inp.shape[0], inp.shape[3], inp.shape[1], inp.shape[2])\n",
    "            pred_out = model(inp)\n",
    "            \n",
    "            pred_out = pred_out.reshape(out.shape[0], out.shape[1] * out.shape[2], out.shape[3])\n",
    "            out = out.reshape(out.shape[0], out.shape[1] * out.shape[2], out.shape[3])\n",
    "            \n",
    "            test_loss += torch.sqrt(criterion(pred_out, out)).item() # sum up batch loss\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \"\"\"Change to the data folder\"\"\"\n",
    "    train_path = \"./new_train/new_train\"\n",
    "    test_path = \"./new_val_in/new_val_in\"\n",
    "    # number of sequences in each dataset\n",
    "    # train:205942  val:3200 test: 36272 \n",
    "    # sequences sampled at 10HZ rate\n",
    "    \n",
    "    # intialize a dataset\n",
    "    val_dataset  = ArgoverseDataset(data_path=train_path)\n",
    "    test_dataset = ArgoverseDataset(data_path=test_path)\n",
    "\n",
    "    TRAIN_SET, TEST_SET = torch.utils.data.random_split(val_dataset, [164753, 41189])\n",
    "\n",
    "    batch_size_train = 16\n",
    "    batch_size_test = 1024\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(TRAIN_SET,batch_size=batch_size_train, shuffle = True, collate_fn=my_collate, num_workers=2,pin_memory=True)\n",
    "    test_loader = DataLoader(TEST_SET,batch_size=batch_size_test, shuffle = True, collate_fn=my_collate, num_workers=2, pin_memory=True)\n",
    "\n",
    "    len(train_loader)\n",
    "\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.5\n",
    "    device = \"cuda\"\n",
    "    model = CNN().cuda() #using cpu here\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=momentum)\n",
    "    num_epoch = 10\n",
    "\n",
    "    for epoch in range(1, num_epoch + 1):\n",
    "            train(model, train_loader, device, optimizer, epoch)\n",
    "            test(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "    inp = [numpy.dstack([scene['p_in'], scene['v_in']]) for scene in batch]\n",
    "    inp = torch.FloatTensor(inp)\n",
    "    return inp\n",
    "\n",
    "t_loader = DataLoader(test_dataset,batch_size=1, shuffle = False, collate_fn=collate, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "header = ['ID']\n",
    "for i in range(1, 61):\n",
    "    header.append('v' + str(i))\n",
    "    \n",
    "with open('sample_submission.csv', 'w') as csvfile: \n",
    "    # creating a csv writer object \n",
    "    csvwriter = csv.writer(csvfile) \n",
    "        \n",
    "    # writing the fields \n",
    "    csvwriter.writerow(header) \n",
    "        \n",
    "    for i_batch, sample_batch in enumerate(t_loader):\n",
    "        header = []\n",
    "        header.append(test_dataset[i_batch]['scene_idx'])\n",
    "        \n",
    "        model.eval()\n",
    "        inp = sample_batch\n",
    "        inp = inp.reshape(inp.shape[0], inp.shape[3], inp.shape[1], inp.shape[2])\n",
    "        inp = inp.to(device)\n",
    "        pred_out = model(inp)\n",
    "        pred_out = pred_out.reshape(1, 60, 30, 2)\n",
    "        pred_out = pred_out.squeeze() \n",
    "        track_id = test_dataset[i_batch]['track_id']\n",
    "        track_id = track_id[:,0,0]\n",
    "        index = 0\n",
    "        for i in range(len(track_id)):\n",
    "            if test_dataset[i_batch]['agent_id'] == track_id[i]:\n",
    "                index = i\n",
    "                break\n",
    "        p_out = pred_out[index]\n",
    "        p_out = p_out.reshape(30*2)\n",
    "        for i in range(len(p_out)):\n",
    "            header.append(p_out[i].item())\n",
    "            \n",
    "        csvwriter.writerow(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "TRAIN_SET, TEST_SET = torch.utils.data.random_split(val_dataset, [164753, 41189])\n",
    "\n",
    "agent_id = 0\n",
    "\n",
    "def show_sample_batch(sample_batch, agent_id):\n",
    "    \"\"\"visualize the trajectory for a batch of samples with a randon agent\"\"\"\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    agent_sz = inp.size(1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1,batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    axs = axs.ravel()   \n",
    "    for i in range(batch_sz):\n",
    "        axs[i].xaxis.set_ticks([])\n",
    "        axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "        axs[i].scatter(inp[i, agent_id,:,0], inp[i, agent_id,:,1])\n",
    "        axs[i].scatter(out[i, agent_id,:,0], out[i, agent_id,:,1])\n",
    "\n",
    "        \n",
    "for i_batch, sample_batch in enumerate(val_loader):\n",
    "    inp, out = sample_batch\n",
    "    \"\"\"TODO:\n",
    "      Deep learning model\n",
    "      training routine\n",
    "    \"\"\"\n",
    "    show_sample_batch(sample_batch, agent_id)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
